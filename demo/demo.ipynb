{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection with SSD\n",
    "### Here we demostrate detection on example images using SSD with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import cv2\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "from ssd import build_ssd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build SSD300 in Test Phase\n",
    "1. Build the architecture, specifyingsize of the input image (300),\n",
    "    and number of object classes to score (21 for VOC dataset)\n",
    "2. Next we load pretrained weights on the VOC0712 trainval dataset  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights into state dict...\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "net = build_ssd('test', 300, 21)    # initialize SSD\n",
    "net.load_weights('../weights/ssd300_mAP_77.43_v2.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Image \n",
    "### Here we just load a sample image from the VOC07 dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 300, 300]           1,792\n",
      "              ReLU-2         [-1, 64, 300, 300]               0\n",
      "            Conv2d-3         [-1, 64, 300, 300]          36,928\n",
      "              ReLU-4         [-1, 64, 300, 300]               0\n",
      "         MaxPool2d-5         [-1, 64, 150, 150]               0\n",
      "            Conv2d-6        [-1, 128, 150, 150]          73,856\n",
      "              ReLU-7        [-1, 128, 150, 150]               0\n",
      "            Conv2d-8        [-1, 128, 150, 150]         147,584\n",
      "              ReLU-9        [-1, 128, 150, 150]               0\n",
      "        MaxPool2d-10          [-1, 128, 75, 75]               0\n",
      "           Conv2d-11          [-1, 256, 75, 75]         295,168\n",
      "             ReLU-12          [-1, 256, 75, 75]               0\n",
      "           Conv2d-13          [-1, 256, 75, 75]         590,080\n",
      "             ReLU-14          [-1, 256, 75, 75]               0\n",
      "           Conv2d-15          [-1, 256, 75, 75]         590,080\n",
      "             ReLU-16          [-1, 256, 75, 75]               0\n",
      "        MaxPool2d-17          [-1, 256, 38, 38]               0\n",
      "           Conv2d-18          [-1, 512, 38, 38]       1,180,160\n",
      "             ReLU-19          [-1, 512, 38, 38]               0\n",
      "           Conv2d-20          [-1, 512, 38, 38]       2,359,808\n",
      "             ReLU-21          [-1, 512, 38, 38]               0\n",
      "           Conv2d-22          [-1, 512, 38, 38]       2,359,808\n",
      "             ReLU-23          [-1, 512, 38, 38]               0\n",
      "           L2Norm-24          [-1, 512, 38, 38]             512\n",
      "        MaxPool2d-25          [-1, 512, 19, 19]               0\n",
      "           Conv2d-26          [-1, 512, 19, 19]       2,359,808\n",
      "             ReLU-27          [-1, 512, 19, 19]               0\n",
      "           Conv2d-28          [-1, 512, 19, 19]       2,359,808\n",
      "             ReLU-29          [-1, 512, 19, 19]               0\n",
      "           Conv2d-30          [-1, 512, 19, 19]       2,359,808\n",
      "             ReLU-31          [-1, 512, 19, 19]               0\n",
      "        MaxPool2d-32          [-1, 512, 19, 19]               0\n",
      "           Conv2d-33         [-1, 1024, 19, 19]       4,719,616\n",
      "             ReLU-34         [-1, 1024, 19, 19]               0\n",
      "           Conv2d-35         [-1, 1024, 19, 19]       1,049,600\n",
      "             ReLU-36         [-1, 1024, 19, 19]               0\n",
      "           Conv2d-37          [-1, 256, 19, 19]         262,400\n",
      "           Conv2d-38          [-1, 512, 10, 10]       1,180,160\n",
      "           Conv2d-39          [-1, 128, 10, 10]          65,664\n",
      "           Conv2d-40            [-1, 256, 5, 5]         295,168\n",
      "           Conv2d-41            [-1, 128, 5, 5]          32,896\n",
      "           Conv2d-42            [-1, 256, 3, 3]         295,168\n",
      "           Conv2d-43            [-1, 128, 3, 3]          32,896\n",
      "           Conv2d-44            [-1, 256, 1, 1]         295,168\n",
      "           Conv2d-45           [-1, 16, 38, 38]          73,744\n",
      "           Conv2d-46           [-1, 84, 38, 38]         387,156\n",
      "           Conv2d-47           [-1, 24, 19, 19]         221,208\n",
      "           Conv2d-48          [-1, 126, 19, 19]       1,161,342\n",
      "           Conv2d-49           [-1, 24, 10, 10]         110,616\n",
      "           Conv2d-50          [-1, 126, 10, 10]         580,734\n",
      "           Conv2d-51             [-1, 24, 5, 5]          55,320\n",
      "           Conv2d-52            [-1, 126, 5, 5]         290,430\n",
      "           Conv2d-53             [-1, 16, 3, 3]          36,880\n",
      "           Conv2d-54             [-1, 84, 3, 3]         193,620\n",
      "           Conv2d-55             [-1, 16, 1, 1]          36,880\n",
      "           Conv2d-56             [-1, 84, 1, 1]         193,620\n",
      "          Softmax-57             [-1, 8732, 21]               0\n",
      "================================================================\n",
      "Total params: 26,285,486\n",
      "Trainable params: 26,285,486\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.03\n",
      "Forward/backward pass size (MB): 415.30\n",
      "Params size (MB): 100.27\n",
      "Estimated Total Size (MB): 516.60\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/torch/csrc/autograd/python_function.cpp:638: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n"
     ]
    }
   ],
   "source": [
    "summary(net, (3, 300, 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSD(\n",
      "  (vgg): ModuleList(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "    (31): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6))\n",
      "    (32): ReLU(inplace=True)\n",
      "    (33): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (34): ReLU(inplace=True)\n",
      "  )\n",
      "  (L2Norm): L2Norm()\n",
      "  (extras): ModuleList(\n",
      "    (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (2): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (3): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (4): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (5): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (6): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "  )\n",
      "  (loc): ModuleList(\n",
      "    (0): Conv2d(512, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): Conv2d(1024, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (2): Conv2d(512, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): Conv2d(256, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (conf): ModuleList(\n",
      "    (0): Conv2d(512, 84, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): Conv2d(1024, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (2): Conv2d(512, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): Conv2d(256, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): Conv2d(256, 84, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): Conv2d(256, 84, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (softmax): Softmax(dim=-1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = cv2.imread('./data/example.jpg', cv2.IMREAD_COLOR)  # uncomment if dataset not downloaded\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from data import VOCDetection, VOC_ROOT, VOCAnnotationTransform\n",
    "# here we specify year (07 or 12) and dataset ('test', 'val', 'train') \n",
    "testset = VOCDetection(VOC_ROOT, [('2007', 'val')], None, VOCAnnotationTransform())\n",
    "img_id = 60\n",
    "image = testset.pull_image(img_id)\n",
    "rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "# View the sampled input image before transform\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(rgb_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process the input.  \n",
    "#### Using the torchvision package, we can create a Compose of multiple built-in transorm ops to apply \n",
    "For SSD, at test time we use a custom BaseTransform callable to\n",
    "resize our image to 300x300, subtract the dataset's mean rgb values, \n",
    "and swap the color channels for input to SSD300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = cv2.resize(image, (300, 300)).astype(np.float32)\n",
    "x -= (104.0, 117.0, 123.0)\n",
    "x = x.astype(np.float32)\n",
    "x = x[:, :, ::-1].copy()\n",
    "plt.imshow(x)\n",
    "x = torch.from_numpy(x).permute(2, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSD Forward Pass\n",
    "### Now just wrap the image in a Variable so it is recognized by PyTorch autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xx = Variable(x.unsqueeze(0))     # wrap tensor in Variable\n",
    "if torch.cuda.is_available():\n",
    "    xx = xx.cuda()\n",
    "y = net(xx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse the Detections and View Results\n",
    "Filter outputs with confidence scores lower than a threshold \n",
    "Here we choose 60% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import VOC_CLASSES as labels\n",
    "top_k=10\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "colors = plt.cm.hsv(np.linspace(0, 1, 21)).tolist()\n",
    "plt.imshow(rgb_image)  # plot the image for matplotlib\n",
    "currentAxis = plt.gca()\n",
    "\n",
    "detections = y.data\n",
    "# scale each detection back up to the image\n",
    "scale = torch.Tensor(rgb_image.shape[1::-1]).repeat(2)\n",
    "for i in range(detections.size(1)):\n",
    "    j = 0\n",
    "    while detections[0,i,j,0] >= 0.6:\n",
    "        score = detections[0,i,j,0]\n",
    "        label_name = labels[i-1]\n",
    "        display_txt = '%s: %.2f'%(label_name, score)\n",
    "        pt = (detections[0,i,j,1:]*scale).cpu().numpy()\n",
    "        coords = (pt[0], pt[1]), pt[2]-pt[0]+1, pt[3]-pt[1]+1\n",
    "        color = colors[i]\n",
    "        currentAxis.add_patch(plt.Rectangle(*coords, fill=False, edgecolor=color, linewidth=2))\n",
    "        currentAxis.text(pt[0], pt[1], display_txt, bbox={'facecolor':color, 'alpha':0.5})\n",
    "        j+=1"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
